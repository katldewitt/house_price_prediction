---
title: "STAT 420 Final Report: Modeling California Housing Prices"
author: "Shashank Thakur, Avinash Tiwari, Kathryn DeWitt"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

# Modeling California Housing Prices

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")

library(ggplot2)
library(faraway)
library(knitr)
library(broom)
library(sf)

green_color <- "#009E73"
```

## Introduction

### Executive Summary

In this report we discuss the creation of a linear model to model median house value of a block group in California (CA) in the 1990's based on administrative (Census) data. Given the recent turbulence in the real estate market and associated bidding wars, there is a renewed interest in what the value of a house is. Utilizing methods from STAT 420, we underwent model selection to identify the optimal model that minimized test RMSE, met the assumptions of linear modelling, and had the fewest parameters to enhance explainability. The best linear model was the full additive model, which included the parameters `longitude`, `latitude`, `housing_median_age`, `total_rooms`, `population`, `median_income`, and `ocean_proximity`.This model had a test RMSE of 0.3407 and an adjusted $R^2$ of 0.7597. Our model provides utility in explaining the relationship between how features of a California Block Group relate to the median house value of the block group.

### The Data

For our project we leveraged a data set from kaggle [California Housing Prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices). The data contains information from the 1990 California census and pertains to houses found in a given California district. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).

The *California Housing Prices* data set pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. There are 10 columns and 20,640 rows in the data. The columns are as mentioned below: 

| **Column Name**    	   | **Description**                                                                                   	|
|------------------------|---------------------------------------------------------------------------------------------------	|
| longitude          	   | Longitudinal location of block group of houses.                                                    |
| latitude           	   | Latitudinal location of block group of houses                                                     	|
| housing_median_age 	   | Median age of the houses in the block group                                                       	|
| total_rooms        	   | Total rooms in the group of houses                                                                	|
| total_bedrooms     	   | Total bedrooms in the block group                                                                 	|
| population         	   | Total number of people in the block group                                                         	|
| households         	   | Households comprised in the block group                                                           	|
| median_income      	   | Median income calculated from the individual income of house.                                     	|
| median_house_value 	   | Median house value of a block group.                                                              	|
| ocean_proximity    	   | Indicating whether each block group is near the ocean, near the Bay Area, inland or on an island. 	|


### Limitations of the Data 

In order to ensure the reader understands the caveats of working with our data, we first want to underscore the differences between census housing data and typical sources of real estate data. The census data set, unlike scraped real estate data, is gathered by an administrative body, the Census. Since the stakeholder involved in collecting the data is not interested in selling a house, we see different features represented that relate more to geography (Latitude, longitude), population density (population, number of households), and age of the building (median_housing_age).The data set is aggregated at the block level (rather than at the house level). This means our analysis was not directly modelling housing values, but the median housing value of a neighborhood. We  believe this helped us better to understand the value of the neighborhood as opposed to the value of the amenities of the individual houses.

Another important limitation of the source of this data is the date range. Since the data were gathered as part of the 1990 Census, we cannot extrapolate to current housing prices or utilize the model to explain what features are important to median house value at any other time period. For this particular reason, we focused our project on modelling rather than prediction. While there are few use cases for a prediction algorithm for median house value of a census block group in the 1990's at the present date, understanding historical features that related to a census block groups' price point provides an opportunity for understanding how median house price varied based on parameters at the time of data collection.

## Methods

We will now discuss the steps we used to generate our linear model. We began by conducting exploratory data analysis (EDA). Based on the EDA, we identified steps necessary to clean our data. In order to ensure our model was not overfit, we split the data set into a 70-30 train-test split. Next we underwent model selection process to minimize the test RMSE of our model. In order to conduct model selection and graph the results, we leveraged helper functions that we will describe before diving into the model selection.


### Helper Functions

**Function   : **`model_performance_metrics` will generate model performance metrics such as RMSE and R-squared, etc.

**Inputs: **

*models      : * A list of models to generate the performance metrics

*model_names : * Names of the models passed to the function

*data        :* The actual data which will be used to calculate predicted values and RMSE for each model

**Output     : ** Table with following performance metrics as columns and a row for each model 
                  provided in `model_names` as input.

                  Adjusted R-Squared
                  Root Mean Square Error on Training Data
                  Root Mean Square Error on Test Data
                  Leave One Out Cross Validated RMSE
                  Total number of coefficients in the model
```{r}
model_performance_metrics = function(models, model_names, data){
  # Calculate model names and total numbers
  n_mod = length(models)
  # Initialize the data frame to be be built by the function
  perf_metrics_df = data.frame(
    row.names = model_names,
    "ADJR2" = rep(0L, n_mod),
    "Train.RMSE" = rep(0L, n_mod),
    "Test.RMSE"  = rep(0L, n_mod),
    "LOOCV_RMSE" = rep(0L, n_mod),
    "Coefs" = rep(0L, n_mod)
  )
  # Extract actual response values
  y_i = log(data[["median_house_value"]])
  
  for (idx in 1:n_mod){
    perf_metrics_df[model_names[idx], "ADJR2"] = summary(models[[idx]])$adj.r.squared 
    perf_metrics_df[model_names[idx], "Train.RMSE"] = sqrt(mean(resid(models[[idx]]) ^ 2)) 
    y_hat = predict(models[[idx]], newdata = data)
    perf_metrics_df[idx, "Test.RMSE"] = sqrt(mean((y_i - y_hat) ^ 2))    
    perf_metrics_df[model_names[idx], "LOOCV_RMSE"] = sqrt(mean((resid(models[[idx]]) / 
                                                      (1 - hatvalues(models[[idx]]))) ^ 2)) 
    perf_metrics_df[model_names[idx], "Coefs"] = length(coef(models[[idx]]))
  }

  # Print Model performance on train data
  kable(perf_metrics_df,
        col.names = c("Adjusted R-Squared",
                      "Train RMSE",
                      "Test RMSE",
                      "LOOCV RMSE", 
                      "Coefficients"),
        caption = "Comparative Model Performance Metrics")

}
```


### Data Cleaning

Our first step to cleaning the data was reading in the CSV file from kaggle.com. Our team members reviewed the first five rows to ensure that there were no encoding issues reading in the raw data into R.

```{r}
#read in the data
read_in <- function(){
  ca_housing_data = read.csv('../000_Data/california-housing-prices/housing.csv')
  ca_housing_data
}

ca_housing_data <- read_in()
head(ca_housing_data)
```


The first thing we we looked at were basic summary statistics and missingness. We found that there were 207 observation missing `total_bedrooms`. We cannot generate a linear model with missing data. However, looking at the collinearity, we will find a solution that allows us to keep these observations.

```{r}
summary(ca_housing_data)

#How many missing values do we have?
colSums(is.na(ca_housing_data))
```

As part of data cleaning, we plotted the `median_house_value` against latitude and longitude to visualize the locations of the different houses. One of the first things we noticed were two distinct clusters of highly priced houses that appear to correspond to Los Angeles and the San Francisco Bay Area in bright red. The lighter green dots in the plot illustrate the lowest priced houses. Note that this scale differentiates by color using the median `median_house_value` as a midpoint, which helps to illustrate concentrated areas of high price values.

```{r}
## Map the data
#Source: https://stackoverflow.com/questions/65233613/plot-a-map-using-lat-and-long-with-r
my_sf <- st_as_sf(ca_housing_data, coords = c('longitude', 'latitude'))
my_sf <- st_set_crs(my_sf, value = 4326)

ggplot(my_sf) + 
  geom_sf(aes(color = median_house_value)) +
              labs( y = "Longitude", 
                    x = "Latitude",
                    title = "Graphic Of Median House Value by Location") +
  scale_colour_gradient2(low = green_color,
    midpoint = median(ca_housing_data$median_house_value),
    high = "#D55E00") 
```

We also generated histograms for all relevant predictors. Please note that not all EDA was included in this report; additional code from EDA can be found on our [GitHub Repo for All Project Related Code]. The linked section includes the link and describes the relevant .rmd files that were out of scope of this report.

The most important finding discovered while performing EDA was the long tail of our outcome variable `median_house_value`. As displayed below in the histogram, we have a number of houses that are at the top end of the range.

```{r}
ggplot(ca_housing_data, aes(median_house_value)) +
  geom_histogram(fill = green_color, color= "#FFFFFF", bins = 15) +
  labs( y = "Count of Block Groups", 
                    x = "Median House Value in USD",
                    title = "Histogram of Median House Values")
```

**Based on the histogram, we see that our response variable could benefit from a log transformation.** Generally most dollar variables require a log transformation, and `median_house_value` is no different. This log transformation will also assist with linear model assumptions during model selection.

```{r}
ggplot(ca_housing_data, aes(log(median_house_value))) +
  geom_histogram(fill = green_color, color= "#FFFFFF", bins = 15) +
  labs( y = "Count of Block Groups", 
                    x = "Median House Value in USD",
                    title = "Histogram of Median House Values")
```

In addition to looking at graphs and reviewing summary statistics, we also began to identify variables with collinearity issues via a correlation matrix and pairwise plot. In the pairwise plot we see that `total_bedrooms` is highly collinear with `total_rooms`, but `total_rooms` is not missing any values. With this in mind, we can utilize the variable `total_rooms` to preserve the 207 observations. 

```{r}
## pairs & Cor matrix
numeric_vals <- unlist(lapply(ca_housing_data, is.numeric), use.names = FALSE)  
pairs(ca_housing_data[,numeric_vals], col=green_color)
round(cor(ca_housing_data[,numeric_vals]), 2)
#Population and households are collinear
#Population and Total rooms are also collinear
#latitude and longitude are also highly correlated

#Remove total_bedrooms
ca_housing_data <- subset(ca_housing_data, select=-c(total_bedrooms))
```

We have another column `ocean_proximity` which is a categorical variable indicating how close the block is from ocean. Using latitude and longitude, we can see how this factor variable helps to classify different census blocks. Keeping in mind that we saw distinct clusters of highly priced median house value in the Bay Area (see the blue "NEAR BAY" region) and LA (around 34N, 118W "NEAR OCEAN" region), this factor along with latitude and longitude will help us to capture the geographic nature of housing value.


```{r}
ggplot(my_sf) + 
  geom_sf(aes(color = ocean_proximity)) +
              labs( y = "Longitude", 
                    x = "Latitude",
                    title = "Ocean Proximity by Location")
```

In the table below, the reader can see this column has 5 different categories. It is important to note that the `ISLAND` category has only 5 observations. During initial analysis, our team determined that `ISLAND` has a substantially higher average value of response variable `median_house_value` than the other categories as illustrated by the below boxplot. This fact is problematic for our purpose. We will be dropping the rows from data set with `ISLAND` category and coercing the predictor `ocean_proximity` to be categorical.

```{r}
#Ocean proximity - only 5 in island, and substantially different. I propose removing from consideration to avoid overfitting depending on an unlucky test/train split
table(ca_housing_data$ocean_proximity) 
boxplot(median_house_value ~ ocean_proximity, 
        ca_housing_data,
        main = "Boxplot of Median House Value by Ocean Proximity",
        ylab = "Median House value in USD",
        xlab = "Ocean Proximity",
        col = green_color)

#Remove the islands rows
ca_housing_data = ca_housing_data[ca_housing_data$ocean_proximity != "ISLAND",]

#Now we turn it into a factor after dropping that level
ca_housing_data$ocean_proximity <- as.factor(ca_housing_data$ocean_proximity)
```

At this point, we have concluded the prerequisite data cleaning based on EDA and can move onto model selection. Our final data set used for modelling has `r nrow(ca_housing_data)` rows and `r ncol(ca_housing_data)` columns. The summary statistics can be found below.


```{r}
#Final dataset and quality assurance
summary(ca_housing_data)
str(ca_housing_data)

#QA: assert no missing values
if(any(colSums(is.na(ca_housing_data)) > 0)){
  stop("Fatal error. Missing data is one or more columns")
}

#QA: Assert that there are only 4 levels in ocean_proximity
if(length(levels(ca_housing_data$ocean_proximity)) != 4){
  stop("Fatal error. There are not 4 levels in ocean_proximity")
}
```


### Model Selection Process

After cleaning the data, we proceeded with building a model. The very first step we took was to divide data into train and test. Based on total number of observations in the data set, team decided to use 70%-30% split for train-test data.


We first initialized global variables and created helper functions (which are listed above in [Helper Functions] above).

```{r}
# build different models to test the performance
n_models = 7
# create a vector list of models to pass to various functions
cahoudta_models = vector("list", length = n_models)
cahoudta_model_names = vector("character", length = n_models)
# Train-Test Split Ration
tr_pct = 0.70
```

In order to replicate the results, we set a seed to ensure that the test train split of our data was consistent between team members. After creating both the test and train dataset, we began building models.

```{r}
set.seed(420072022)
# Randomly select train indexes
cahoudta_trn_idx = sample(nrow(ca_housing_data), 
                          size = trunc(tr_pct * nrow(ca_housing_data)))
# Split into train and test data sets based on the index
ca_housing_data.tr = ca_housing_data[cahoudta_trn_idx, ]
ca_housing_data.te = ca_housing_data[-cahoudta_trn_idx, ]

n_tr_rows = nrow(ca_housing_data.tr)
n_te_rows = nrow(ca_housing_data.te)
```

In the process of model selection, we built a total of 8 different models. While recursively analyzing different model predictors and transformations, our team identified that there are few outlier observations which result in issues with the model training. So we circled back to the very first full additive model and identified these outliers using Cook's Distance.

```{r}
cahoudta_full_add_model_cd = lm(log(median_house_value) ~ ., data = ca_housing_data.tr)
cd_full_add_mod = cooks.distance(cahoudta_full_add_model_cd)
(count_outliers = sum(cd_full_add_mod > 4 / length(cd_full_add_mod)))
percent_outliers = count_outliers / nrow(ca_housing_data.tr) 
```

There were a total of `r count_outliers` observations deemed as influential based on the heuristic $D_i > \frac{4}{n}$ where $D_i$ is the distance of the $i$th observation and $n$ is the number of observations. Given that these outliers represent only `r scales::percent(percent_outliers)` of the training data, our team decided to drop these outliers from the training dataset.

When we re-fit the full additive model by ignoring these **`r count_outliers`** outliers we found that ultimately this model supplied the best Test RMSE.

```{r}
cahoudta_full_add_model = lm(log(median_house_value) ~ ., 
                             data = ca_housing_data.tr,
                             subset = (cd_full_add_mod <= 4 / length(cd_full_add_mod)))
```

Checking if there is multi-collinearity in the full fitted additive model. Following code snippet outputs all the predictors which has high variation inflation factors.
```{r}
high_vif_predictors = vif(cahoudta_full_add_model) > 5
high_vif_pred_vals = vif(cahoudta_full_add_model)[high_vif_predictors]
sort(high_vif_pred_vals)
```

Based on variable inflation factors the predictors `households` and `latitude` can be dropped. 

Lets try backward BIC search from the full additive model.
```{r}
cahoudta_add_bckwd_bic_model = step(cahoudta_full_add_model, 
                                    direction = "backward",
                                    k = log(n_tr_rows),
                                    trace = 0)
```

Lets try two way interaction full additive model.
```{r}
cahoudta_2int_add_model = lm(log(median_house_value) ~ . ^ 2, 
                             data = ca_housing_data.tr)

```

Lets try backward BIC search from two way interaction full additive model.
```{r}
cahoudta_2int_bckwd_bic_model = step(cahoudta_2int_add_model, 
                                    direction = "backward",
                                    k = log(n_tr_rows),
                                    trace = 0)

```

We will run pairs plot to check any obvious trends in predictors compared to the response variable `median_house_value`.
```{r fig.height=8, fig.width=12}
pairs(subset(ca_housing_data.tr, 
             select = -ocean_proximity), 
      col = "orchid2")
```

Based on pairs plot and for response variable `median_house_value`

- Variables `median_income`, `households`, `total_rooms` have some polynomial relationship.

- Variable `housing_median_age` has lot of variance and needs variable transformation.

- Either one of `latitude` or `longitude` can be eliminated due to high partial collinearity.


Lets drop the predictors `households` and `latitude` with high VIF.
```{r}
predictor_variables = colnames(ca_housing_data.tr)[-which(colnames(ca_housing_data.tr)
                                                          == "median_house_value")]
drop_cols = which(colnames(ca_housing_data.tr) == "median_house_value" | 
                    colnames(ca_housing_data.tr) == "households" | 
                    colnames(ca_housing_data.tr) == "latitude")
smaller_predictors = colnames(ca_housing_data.tr)[-drop_cols]
less_preds = paste0("log(median_house_value) ~ ", 
                    paste(smaller_predictors, collapse = " + "))
cahoudta_lowvif_add_mod = lm(formula = less_preds, data = ca_housing_data.tr)
```

Lets add 2 way interaction for above model to check the performance.
```{r}
less_preds_2int = paste0("log(median_house_value) ~ (", 
                         paste(smaller_predictors, collapse = " + "), ") ^ 2")
cahoudta_lowvif_2int_mod = lm(formula = less_preds_2int, data = ca_housing_data.tr)
```

To increase complexity, lets try a degree 2 polynomial for reduced predictors based on vif above. Also we have to exclude the categorical predictor `ocean_proximity` from polynomial formula.
```{r}
drop_cols = which(colnames(ca_housing_data.tr) == "median_house_value" | 
                    colnames(ca_housing_data.tr) == "households" | 
                    colnames(ca_housing_data.tr) == "latitude")
reduced_predictors = colnames(ca_housing_data.tr)[-drop_cols]
less_preds = paste0("log(median_house_value) ~ ", paste(smaller_predictors, 
                                                        collapse = " + "))
further_reduced_predictors = reduced_predictors[-which(reduced_predictors 
                                                       == "ocean_proximity")]
less_poly_preds = paste0(" + I(", paste(further_reduced_predictors, 
                                        collapse = " ^ 2) + I("), " ^ 2)")
poly_formula = paste0(less_preds, less_poly_preds)
message("Polynomial Formula: ", poly_formula)
cahoudta_lowvif_2poly_mod = lm(formula = poly_formula, data = ca_housing_data.tr)
# summary(cahoudta_lowvif_2poly_mod)$coef
```

I will add back `latitude` and test if that gives better results.
```{r}
drop_cols = which(colnames(ca_housing_data.tr) == "median_house_value" | 
                    colnames(ca_housing_data.tr) == "households")
lesser_predictors = colnames(ca_housing_data.tr)[-drop_cols]
less_preds = paste0("log(median_house_value) ~ ", 
                    paste(lesser_predictors, 
                          collapse = " + "))
message("Model 8: Formula: ", less_preds)
cahoudta_lola_add_mod = lm(formula = less_preds, data = ca_housing_data.tr)
```

Next we will transform predictors `population` and `total_rooms` to be normalized with predictor `households` and build a new model.
```{r}
ca_housing_data.tr$pop_per_hh = (ca_housing_data.tr$population / 
                                   ca_housing_data.tr$households)
ca_housing_data.tr$rooms_per_hh = (ca_housing_data.tr$total_rooms / 
                                     ca_housing_data.tr$households)
ca_housing_data.te$pop_per_hh = (ca_housing_data.te$population / 
                                   ca_housing_data.te$households)
ca_housing_data.te$rooms_per_hh = (ca_housing_data.te$total_rooms / 
                                     ca_housing_data.te$households)
```

Fit a additive model with transformed variables and predictors without high variation inflation factors or collinearity.
```{r}
cahoudta_2int_txfd_model_cd = lm(log(median_house_value) ~ 
                                (longitude + 
                                   housing_median_age +
                                   median_income + 
                                   pop_per_hh + 
                                   rooms_per_hh +
                                   ocean_proximity ) ^ 2,
                              data = ca_housing_data.tr)
cd_2int_txfd = cooks.distance(cahoudta_2int_txfd_model_cd)
cahoudta_2int_txfd_model = lm(log(median_house_value) ~ 
                                (longitude + 
                                   housing_median_age +
                                   median_income + 
                                   pop_per_hh + 
                                   rooms_per_hh +
                                   ocean_proximity ) ^ 2,
                              data = ca_housing_data.tr,
                              subset = (cd_2int_txfd <= 4 / length(cd_2int_txfd)))
```

### Comparative Model Performance Metrics

We will now compare all the models to calculate and display the performance metrics.
```{r}
cahoudta_models[[1]] = cahoudta_full_add_model
cahoudta_model_names[[1]] = "cahoudta_full_add_model"

cahoudta_models[[2]] = cahoudta_add_bckwd_bic_model
cahoudta_model_names[[2]] = "cahoudta_add_bckwd_bic_model"

cahoudta_models[[3]] = cahoudta_2int_add_model
cahoudta_model_names[[3]] = "cahoudta_2int_add_model"

cahoudta_models[[4]] = cahoudta_2int_bckwd_bic_model
cahoudta_model_names[[4]] = "cahoudta_2int_bckwd_bic_model"

cahoudta_models[[5]] = cahoudta_lowvif_add_mod
cahoudta_model_names[[5]] = "cahoudta_lowvif_add_mod"

cahoudta_models[[6]] = cahoudta_lowvif_2int_mod
cahoudta_model_names[[6]] = "cahoudta_lowvif_2int_mod"

cahoudta_models[[7]] = cahoudta_lowvif_2poly_mod
cahoudta_model_names[[7]] = "cahoudta_lowvif_2poly_mod"

cahoudta_models[[8]] = cahoudta_lola_add_mod
cahoudta_model_names[[8]] = "cahoudta_lola_add_mod"

cahoudta_models[[9]] = cahoudta_2int_txfd_model
cahoudta_model_names[[9]] = "cahoudta_2int_txfd_model"

# Print Comparative Model performance on train data
model_performance_metrics(cahoudta_models, 
                          cahoudta_model_names, 
                          ca_housing_data.te)
```

As we can see from the results that the backward BIC search doesn't help in reducing number of beta parameters. Both the full model `cahoudta_full_add_model` and backward step wise search model `cahoudta_add_bckwd_bic_model` has same number of parameters i.e. 11. So we tried to reduce the train-test split to 50%-50%. Even after that the backward BIC search reduced only 1 parameter from 11 to 10 with very negligible loss in adjusted r-squared.

Having both `latitude` and `longitude` reduces the adjusted r-squared further and is not a good predictor combination.


## Model Selection

## Results

During the model building process, we explored **`r n_models`** different models for our housing data set. Recall that he main goal for this project was to effectively model the California Housing Data and identify the predictors which help explain the `median_house_value` response variable. Based on table [Comparative Model Performance Metrics](#comparative-model-performance-metrics) we select model `cahoudta_full_add_model` as our best model for purpose of this project. 

The final selected model has following features:

- High `adjusted r-squared` of **0.7597**
- Lowest `Test RMSE` of **0.3407**
- Lowest `Train RMSE` of **0.2687**
- Due to low number of coefficients **(11)** it is easy to interpret and computationally efficient.

The following are all the predictors for the model `cahoudta_full_add_model` with test statistics and p-values.

```{r}
coef_matrix <- data.frame(coef(summary(cahoudta_full_add_model)))
names(coef_matrix) <- c("Estimate", "Standard Error", "t Value", "P Value")
kable(coef_matrix, digits = 5, caption = "Table of Predictors and Test Statistics")
```

There are several things to notice about the above table when discussing the model. The first is to note that our base level for `ocean_proximity` is "<1H OCEAN". The implication is that the average `median_house_value` of a block group decreases for any other `ocean_proximity`.   

```{r}
coefs_exponeniated <- (exp(coef(cahoudta_full_add_model)) - 1) * 100
```


We also ran all the standard diagnostics on this model.

```{r fig.height=12, fig.width=12}
par(mfrow=c(2, 2))

plot(cahoudta_full_add_model,
     pch = 20,
     col = green_color)
```


### Chosen Model

## Discussion

### Utility of Model

> section should contain discussion of your results and should frame your results in the context of the data. How is your final model useful?

### Future Directions and Lessons Learned

<!TODO: Avi to add other directions we did not have time to implement>

## Appendix 

### Additional Code Used in Model Selection

<!TODO: add details in the appendix on model selection and possibly EDA>

### GitHub Repo for All Project Related Code

### Authors

Team Member's Names: 

- Kathryn DeWitt
- Shashank Thakur
- Avinash Tiwari

NetIDs: 

- kdewitt3
- sthakur5
- tiwari6
